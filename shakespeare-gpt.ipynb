{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Building a GPT from Scratch - With Explanations\n",
        "\n",
        "This notebook provides a step-by-step explanation of Andrej Karpathy's GPT implementation from his [Zero To Hero](https://karpathy.ai/zero-to-hero.html) series. We'll break down each component of the transformer architecture and explain the code in detail.\n",
        "\n",
        "Developed By Eiliya Mohebi For Education Purposes.\n",
        "\n",
        "## What is GPT?\n",
        "\n",
        "GPT (Generative Pre-trained Transformer) is an autoregressive language model that uses the transformer architecture to generate text. It predicts the next token in a sequence given the previous tokens, and can be used for a variety of natural language processing tasks.\n",
        "\n",
        "This implementation is a minimal version that demonstrates the core concepts of transformer-based models.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data Acquisition\n",
        "\n",
        "We start by downloading a dataset to train our model. We'll use the \"Tiny Shakespeare\" dataset - a collection of Shakespeare's works that is commonly used for text generation tasks because it's relatively small but contains enough structure for interesting results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'wget' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "# Download the tiny Shakespeare dataset\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The command above downloads a text file containing Shakespeare's works. If you're running this locally and `wget` isn't available, you can download the file manually from the URL.\n",
        "\n",
        "## 2. Data Exploration\n",
        "\n",
        "Let's load the data and explore its contents to understand what we're working with."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read the text file\n",
        "with open('shakespeare_text', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We open the file with UTF-8 encoding to properly handle all characters, then read the entire text into memory. This is feasible because the dataset is small (about 1MB), but for larger datasets, you might need to process the data in chunks.\n",
        "\n",
        "Let's check the size of our dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length of dataset in characters:  1115394\n"
          ]
        }
      ],
      "source": [
        "print(\"Length of dataset in characters: \", len(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This shows us the total number of characters in our dataset. For language models, size matters - larger datasets generally lead to better models, but they also require more computational resources to train.\n",
        "\n",
        "Let's see what the data looks like by printing the first 1000 characters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Look at the first 1000 characters\n",
        "print(text[:1000])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that the text is in the format of a play script, with character names followed by their lines. This structure will be something our model might learn to mimic.\n",
        "\n",
        "## 3. Tokenization - Creating a Vocabulary\n",
        "\n",
        "Before we can process text with a neural network, we need to convert it to numbers. We'll use a simple character-level tokenization scheme, where each unique character is assigned an integer ID."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "Vocabulary size: 65\n"
          ]
        }
      ],
      "source": [
        "# Get all unique characters to create our vocabulary\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(\"Vocabulary size:\", vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we:\n",
        "1. Use Python's `set()` function to find all unique characters in the text\n",
        "2. Convert the set to a sorted list to ensure the character order is consistent across runs\n",
        "3. Count them to get our vocabulary size\n",
        "4. Print all characters to see what's in our vocabulary\n",
        "\n",
        "Character-level tokenization is simple and works well for small datasets. More advanced models like GPT-3 use subword tokenization methods like Byte-Pair Encoding (BPE), which can handle the trade-off between character and word-level tokenization more efficiently.\n",
        "\n",
        "Now, let's create mappings to convert between characters and their IDs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[46, 43, 50, 50, 53, 1, 61, 53, 56, 50, 42]\n",
            "hello world\n"
          ]
        }
      ],
      "source": [
        "# Create mappings between characters and their IDs\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }  # string to integer\n",
        "itos = { i:ch for i,ch in enumerate(chars) }  # integer to string\n",
        "\n",
        "# Define encode and decode functions\n",
        "encode = lambda s: [stoi[c] for c in s]  # Convert string to list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l])  # Convert list of integers back to string\n",
        "\n",
        "# Test our encoding/decoding\n",
        "print(encode(\"hello world\"))\n",
        "print(decode(encode(\"hello world\")))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We've created two dictionaries:\n",
        "- `stoi` (string to integer): Maps each character to its numeric ID\n",
        "- `itos` (integer to string): Maps each ID back to its character\n",
        "\n",
        "We then define two lambda functions:\n",
        "- `encode`: Converts a string to a list of integer IDs\n",
        "- `decode`: Converts a list of integer IDs back to a string\n",
        "\n",
        "The test at the end confirms that we can encode and decode correctly - we should get back exactly what we put in.\n",
        "\n",
        "## 4. Converting Text to Tensors\n",
        "\n",
        "Now we'll convert our entire text dataset into PyTorch tensors, which are multidimensional arrays optimized for neural network operations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1115394]) torch.int64\n",
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
          ]
        }
      ],
      "source": [
        "# Convert text to tensor\n",
        "import torch  # Import PyTorch\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:100])  # Print the first 100 tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here, we encode the entire text into a list of integers and convert it to a PyTorch tensor. We specify the data type as `torch.long` (a 64-bit integer) to ensure compatibility with PyTorch's embedding layers later.\n",
        "\n",
        "The `.shape` attribute tells us the dimensions of our tensor (in this case, a 1D array with length equal to the number of characters in our text), and `.dtype` confirms the data type.\n",
        "\n",
        "## 5. Train/Validation Split\n",
        "\n",
        "A standard practice in machine learning is to split your data into training and validation sets. We'll use the training set to update our model's weights, and the validation set to evaluate the model's performance on unseen data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training data length: 1003854\n",
            "Validation data length: 111540\n"
          ]
        }
      ],
      "source": [
        "# Split into training and validation sets\n",
        "n = int(0.9 * len(data))  # Use 90% for training, 10% for validation\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "print(f\"Training data length: {len(train_data)}\")\n",
        "print(f\"Validation data length: {len(val_data)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We allocate 90% of our data for training and the remaining 10% for validation. This split is important for monitoring overfitting - if the model performs much better on the training data than on the validation data, it may be memorizing the training data rather than learning general patterns.\n",
        "\n",
        "## 6. Context Windows and Training Examples\n",
        "\n",
        "Language models work by predicting the next token given a context of previous tokens. We need to define how large this context window should be."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])\n"
          ]
        }
      ],
      "source": [
        "# Define context size\n",
        "block_size = 8  # Maximum context length for predictions\n",
        "print(train_data[:block_size+1])  # Show an example context + target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we set `block_size = 8`, which means our model will use at most 8 previous tokens as context when predicting the next token. This parameter is often called the \"context window\" or \"sequence length\" and is an important hyperparameter in transformer models.\n",
        "\n",
        "Let's visualize how we create training examples using this context window:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "When input is [18] the target is: 47 ('i')\n",
            "When input is [18, 47] the target is: 56 ('r')\n",
            "When input is [18, 47, 56] the target is: 57 ('s')\n",
            "When input is [18, 47, 56, 57] the target is: 58 ('t')\n",
            "When input is [18, 47, 56, 57, 58] the target is: 1 (' ')\n",
            "When input is [18, 47, 56, 57, 58, 1] the target is: 15 ('C')\n",
            "When input is [18, 47, 56, 57, 58, 1, 15] the target is: 47 ('i')\n",
            "When input is [18, 47, 56, 57, 58, 1, 15, 47] the target is: 58 ('t')\n"
          ]
        }
      ],
      "source": [
        "# Demonstrate how training examples are created\n",
        "x = train_data[:block_size]  # Input context\n",
        "y = train_data[1:block_size+1]  # Target (next token for each position)\n",
        "\n",
        "for t in range(block_size):\n",
        "    context = x[:t+1]\n",
        "    target = y[t]\n",
        "    print(f\"When input is {context.tolist()} the target is: {target.item()} ('{decode([target.item()])}')\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This example demonstrates how we create training examples for our language model:\n",
        "\n",
        "1. For each position `t` in our sequence, we take all tokens from the beginning up to position `t` as our context\n",
        "2. The target to predict is the token at position `t+1`\n",
        "\n",
        "For example:\n",
        "- Given context \"F\", predict \"i\"\n",
        "- Given context \"Fi\", predict \"r\"\n",
        "- Given context \"Fir\", predict \"s\"\n",
        "- And so on...\n",
        "\n",
        "This is how autoregressive language models work - they predict one token at a time, using all previous tokens as context.\n",
        "\n",
        "## 7. Batch Processing\n",
        "\n",
        "To train efficiently, we process multiple sequences in parallel rather than one at a time. Let's implement a function to generate batches of data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input shapes: torch.Size([4, 8]) torch.Size([4, 8])\n",
            "Input examples:\n",
            "[34, 43, 56, 63, 1, 61, 43, 50] → [43, 56, 63, 1, 61, 43, 50, 50]\n",
            "[56, 42, 52, 43, 57, 57, 10, 1] → [42, 52, 43, 57, 57, 10, 1, 58]\n",
            "[0, 26, 53, 61, 1, 19, 53, 42] → [26, 53, 61, 1, 19, 53, 42, 1]\n",
            "[21, 33, 31, 10, 0, 25, 39, 49] → [33, 31, 10, 0, 25, 39, 49, 43]\n"
          ]
        }
      ],
      "source": [
        "# Function to get random training batches\n",
        "def get_batch(split):\n",
        "    # Choose the appropriate data source\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    \n",
        "    # Generate random starting indices\n",
        "    ix = torch.randint(len(data) - block_size, (4,))\n",
        "    \n",
        "    # Create batch tensors for inputs and targets\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    \n",
        "    return x, y\n",
        "\n",
        "# Test our batch generation\n",
        "xb, yb = get_batch('train')\n",
        "print('Input shapes:', xb.shape, yb.shape)\n",
        "print('Input examples:')\n",
        "for b in range(4):\n",
        "    print(xb[b].tolist(), '→', yb[b].tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This `get_batch` function generates training or validation batches:\n",
        "\n",
        "1. It selects data from either the training or validation set based on the `split` parameter\n",
        "2. It randomly samples 4 starting indices using `torch.randint`\n",
        "3. For each starting index `i`, it creates:\n",
        "   - An input sequence `x` containing tokens from position `i` to `i+block_size-1`\n",
        "   - A target sequence `y` containing tokens from position `i+1` to `i+block_size`\n",
        "4. It stacks these sequences into batches using `torch.stack`\n",
        "\n",
        "The result is two tensors with shape (batch_size, block_size), where:\n",
        "- `batch_size = 4` (the number of sequences processed in parallel)\n",
        "- `block_size = 8` (the length of each sequence)\n",
        "\n",
        "This batched processing is crucial for training efficiency. Rather than processing one sequence at a time, we process multiple sequences in parallel, which leverages modern hardware capabilities.\n",
        "\n",
        "## 8. Model Architecture - The Bigram Language Model\n",
        "\n",
        "We'll start with the simplest possible language model - a bigram model. This model only considers the immediate previous token when predicting the next one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# Simple bigram language model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "    \n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # Each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "        \n",
        "    def forward(self, idx, targets=None):\n",
        "        # idx and targets are both (B,T) tensors of integers\n",
        "        logits = self.token_embedding_table(idx)  # (B,T,vocab_size)\n",
        "        \n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            # Reshape logits for the loss function\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            \n",
        "            # Calculate cross entropy loss\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "            \n",
        "        return logits, loss\n",
        "    \n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # Get predictions\n",
        "            logits, _ = self(idx)\n",
        "            \n",
        "            # Focus only on the last time step\n",
        "            logits = logits[:, -1, :]  # (B, C)\n",
        "            \n",
        "            # Apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1)  # (B, C)\n",
        "            \n",
        "            # Sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
        "            \n",
        "            # Append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
        "            \n",
        "        return idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This `BigramLanguageModel` class defines our initial model architecture using PyTorch's neural network framework. Let's break it down:\n",
        "\n",
        "### Architecture Components\n",
        "\n",
        "1. **Embedding Layer**:\n",
        "   ```python\n",
        "   self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "   ```\n",
        "   \n",
        "   This creates a lookup table where each token ID maps to a vector of size `vocab_size`. In a bigram model, this is equivalent to a table of probabilities for each token following each other token. For a more typical language model, the embedding dimension would be smaller than the vocabulary size and followed by additional layers.\n",
        "\n",
        "2. **Forward Pass**:\n",
        "   The `forward` method defines what happens when we pass inputs through the model:\n",
        "   - Inputs `idx` (shape `[B,T]`) are passed through the embedding layer to get `logits` (shape `[B,T,C]`)\n",
        "   - If targets are provided, we calculate the loss using cross-entropy\n",
        "   - Cross-entropy loss measures how well our predictions match the true next tokens\n",
        "\n",
        "   The shapes represent:\n",
        "   - `B`: Batch size (number of sequences processed in parallel)\n",
        "   - `T`: Time/sequence length (context window size)\n",
        "   - `C`: Channel dimension (vocabulary size in this case)\n",
        "\n",
        "3. **Generation Method**:\n",
        "   The `generate` method allows the model to create new text:\n",
        "   - It starts with an initial context `idx`\n",
        "   - For each new token:\n",
        "     - It gets predictions (logits) from the model\n",
        "     - It focuses on the last token's predictions\n",
        "     - It converts logits to probabilities using softmax\n",
        "     - It randomly samples the next token based on these probabilities\n",
        "     - It appends this token to the context\n",
        "   - This process repeats until `max_new_tokens` are generated\n",
        "\n",
        "This is the essence of autoregressive text generation - using the model's predictions to generate each token, then including that token in the context for the next prediction.\n",
        "\n",
        "Let's instantiate our model and try generating some text before training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch shape: torch.Size([4, 8])\n",
            "Logits shape: torch.Size([32, 65])\n",
            "Initial loss: 4.7265\n"
          ]
        }
      ],
      "source": [
        "# Create a model instance\n",
        "model = BigramLanguageModel(vocab_size)\n",
        "\n",
        "# Test the loss calculation\n",
        "xb, yb = get_batch('train')\n",
        "logits, loss = model(xb, yb)\n",
        "print(f\"Batch shape: {xb.shape}\")\n",
        "print(f\"Logits shape: {logits.shape}\")\n",
        "print(f\"Initial loss: {loss.item():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The loss value gives us a measure of how well our model is doing. With an untrained model, we expect the loss to be around `ln(vocab_size)` (~5.3 for a vocabulary of ~65 tokens), which means the model is essentially making random guesses.\n",
        "\n",
        "Let's try generating some text with our untrained model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "UNDQtYi V\n",
            " p-PPidfSV!UVK:'TXVqoIT:,;wvTd?DCAD!bmzZLsjN; VfHoI uaJ3aevq&wTHEvAkse!rOhFBykdRrLeIzkdKHIJeoO;McjTd3F V-PgXGv'tRSZHRZCFd.WcGyJ.3vqkl!YcAeHTq!rG;LZMGgy.ZQMCvxEEbh$okOBdGbfOf\n",
            "oD-hk?SWSRcj;mfAE.bIGY&,-xobfwvMmpu3FvqMhW$yUlAfzt$a3vH!KtMGzULD oem:$sg3!KQnDynQY.bVDsik;M-ZVjT!fTV:ADgMesHXb':YT&PCCeM&:Sd.yCpum3u3vkqpuJ:N.zUct\n",
            "fcA,XCwk\n",
            " JGdCH?-rXV!yMoDCF pet$yMnDCJLiMtRAL3LqFP DD.3 M L:yrMAUX!Ks$WfCV BVd zfZK3Qcz&OZKopwHwpCv'sCw;w\n",
            "kdUNqZLsN.bQ3jT?HOy,hZKQ:h\n",
            "RefcNw?HtJFiKXW!OYqHM;rN;jTfnuq-aL:3\n"
          ]
        }
      ],
      "source": [
        "# Generate from the untrained model\n",
        "context = torch.zeros((1, 1), dtype=torch.long)  # Start with a single token [0]\n",
        "generated_text = model.generate(context, max_new_tokens=500)[0].tolist()\n",
        "print(decode(generated_text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since the model is untrained, the generated text should look like random characters. Each token is sampled independently without considering the context (other than the immediately preceding token), resulting in gibberish.\n",
        "\n",
        "## 9. Training Loop\n",
        "\n",
        "Now let's train our model using gradient descent. We'll use the AdamW optimizer, which is a common choice for transformer models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 0: Loss 5.0017\n",
            "Step 1000: Loss 4.4967\n",
            "Step 2000: Loss 3.6508\n",
            "Step 3000: Loss 3.2955\n",
            "Step 4000: Loss 2.7469\n",
            "Step 5000: Loss 2.6691\n",
            "Step 6000: Loss 3.1093\n",
            "Step 7000: Loss 2.5971\n",
            "Step 8000: Loss 2.7279\n",
            "Step 9000: Loss 2.7960\n"
          ]
        }
      ],
      "source": [
        "# PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "\n",
        "# Training loop\n",
        "batch_size = 32  # Increase batch size for training\n",
        "for steps in range(10000):\n",
        "    # Sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "    \n",
        "    # Evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    \n",
        "    # Zero the gradients from the previous step\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    \n",
        "    # Backward pass to calculate gradients\n",
        "    loss.backward()\n",
        "    \n",
        "    # Update model parameters\n",
        "    optimizer.step()\n",
        "    \n",
        "    # Print progress occasionally\n",
        "    if steps % 1000 == 0:\n",
        "        print(f\"Step {steps}: Loss {loss.item():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This training loop implements the basic procedure for training neural networks:\n",
        "\n",
        "1. **Batch Sampling**: Get a random batch of inputs (`xb`) and targets (`yb`)\n",
        "2. **Forward Pass**: Feed the inputs through the model to get predictions and calculate the loss\n",
        "3. **Gradient Calculation**: Use `loss.backward()` to compute gradients of the loss with respect to model parameters\n",
        "4. **Parameter Update**: Apply the gradients to update model parameters using the optimizer\n",
        "\n",
        "The `optimizer.zero_grad()` call is necessary to clear the gradients from the previous step, as PyTorch accumulates gradients by default.\n",
        "\n",
        "The training process iteratively minimizes the loss function, which means the model's predictions get closer to the actual next tokens in our text.\n",
        "\n",
        "Let's check our progress by evaluating the model on the validation set and generating some text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'train': tensor(2.5267), 'val': tensor(2.5597)}\n"
          ]
        }
      ],
      "source": [
        "# Evaluate loss on validation set\n",
        "@torch.no_grad()  # Disable gradient tracking for efficiency\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(100)\n",
        "        for k in range(100):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()  # Set model back to training mode\n",
        "    return out\n",
        "\n",
        "# Check loss before further training\n",
        "print(estimate_loss())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `estimate_loss` function computes the average loss over multiple batches for both the training and validation sets. This gives us a more reliable estimate of how well our model is performing.\n",
        "\n",
        "We use several important PyTorch features here:\n",
        "\n",
        "1. `@torch.no_grad()` decorator: Disables gradient computation, which saves memory and speeds up evaluation\n",
        "2. `model.eval()`: Sets the model to evaluation mode (disables dropout, etc.)\n",
        "3. `model.train()`: Sets the model back to training mode\n",
        "\n",
        "Now let's generate some text with our trained model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Ansh w,\n",
            "\n",
            "bl:3That?y at bullacam,\n",
            "\n",
            "PEREFcofare lsth a, on.\n",
            "AROP thed Ed,\n",
            "Tho;mave.\n",
            "Hos INasithy mpitheere,\n",
            "Wan th henanoer f GBunau:\n",
            "WBO:\n",
            "TIAr d?\n",
            "Sis try word le, ave;\n",
            "\n",
            "MOMPARaukiad brwhandst'T:\n",
            "A! ghedspzcllimukenouthivef w, che hisot.yc ther tetVMan u oore. sghe, rkidld mf! ftheckerin our ounr?\n",
            "BN:\n",
            "FY' an.\n",
            "\n",
            "mwnnd ICHAs ps RSfrisonduredsly pad PEXYgorse h sstorothefot ifot r Peth purllor Re, IDke blrHRS:\n",
            "IOf;\n",
            "So t t?ghay hurst-&G;\n",
            "O:\n",
            "\n",
            "Aly nd ofurinknowif,\n",
            "NTinghorew malour winn romfe thame;BRot \n"
          ]
        }
      ],
      "source": [
        "# Generate from the trained model\n",
        "context = torch.zeros((1, 1), dtype=torch.long)\n",
        "print(decode(model.generate(context, max_new_tokens=500)[0].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The generated text should now show some patterns typical of Shakespeare's writing, though it will still be quite simple and might not make much sense. This is because our model is a bigram model that only considers the previous token when predicting the next one.\n",
        "\n",
        "To create a more capable model, we need to incorporate a longer context - that's where the transformer architecture comes in!\n",
        "\n",
        "## 10. Introduction to Self-Attention\n",
        "\n",
        "The key innovation in transformers is the self-attention mechanism, which allows the model to consider all previous tokens in the sequence, not just the most recent one. Let's explore how self-attention works through some examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The Mathematical Trick in Self-Attention\n",
        "\n",
        "Self-attention is a mechanism that allows a model to focus on different parts of the input sequence when generating an output. It's one of the core innovations that made transformers so effective for natural language processing tasks.\n",
        "\n",
        "## Understanding Self-Attention\n",
        "\n",
        "In traditional sequence models like RNNs and LSTMs, information flows sequentially through the network, making it difficult to capture long-range dependencies. Self-attention solves this problem by directly connecting each position in the sequence with every other position, allowing information to flow more freely.\n",
        "\n",
        "The key insight is that each token in the sequence can \"attend\" to all previous tokens, giving more weight to some and less to others based on relevance. This weighted aggregation lets the model capture complex patterns and relationships in the data.\n",
        "\n",
        "Some key advantages of self-attention:\n",
        "1. It captures long-range dependencies without the vanishing gradient problems of RNNs\n",
        "2. It can be parallelized, making training much faster\n",
        "3. It provides interpretable attention weights that show which inputs the model is focusing on\n",
        "\n",
        "Let's explore self-attention through a series of examples, starting with a simple demonstration of weighted aggregation using matrix multiplication."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Matrix A (weights):\n",
            "tensor([[1.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333]])\n",
            "\n",
            "Matrix B (values):\n",
            "tensor([[7., 7.],\n",
            "        [7., 6.],\n",
            "        [5., 4.]])\n",
            "\n",
            "Matrix C (weighted aggregation of B):\n",
            "tensor([[7.0000, 7.0000],\n",
            "        [7.0000, 6.5000],\n",
            "        [6.3333, 5.6667]])\n"
          ]
        }
      ],
      "source": [
        "# Toy example of matrix multiplication for weighted aggregation\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Create a \"weights\" matrix (lower triangular to represent causal attention)\n",
        "a = torch.tril(torch.ones(3, 3))\n",
        "# Normalize the weights (for each row, divide by the sum of weights in that row)\n",
        "a = a / a.sum(1, keepdim=True)\n",
        "\n",
        "# Create a \"values\" matrix with random data\n",
        "b = torch.randint(0, 10, (3, 2)).float()\n",
        "\n",
        "# Perform weighted aggregation using matrix multiplication\n",
        "c = a @ b  # Matrix multiplication: (3,3) @ (3,2) -> (3,2)\n",
        "\n",
        "print(\"Matrix A (weights):\")\n",
        "print(a)\n",
        "print(\"\\nMatrix B (values):\")\n",
        "print(b)\n",
        "print(\"\\nMatrix C (weighted aggregation of B):\")\n",
        "print(c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Matrix Multiplication for Weighted Aggregation\n",
        "\n",
        "The example above demonstrates the core mathematical operation in self-attention: weighted aggregation through matrix multiplication. Let's break it down:\n",
        "\n",
        "1. **Matrix A (Weights)**:\n",
        "   - We created a lower triangular matrix using `torch.tril`, where each position can only attend to previous positions (causal attention)\n",
        "   - We normalized the rows so each row sums to 1, turning them into proper attention weights\n",
        "   - Shape: (3, 3) - each row represents the attention weights for a position\n",
        "\n",
        "2. **Matrix B (Values)**:\n",
        "   - Random values representing the information to be aggregated\n",
        "   - Shape: (3, 2) - each row represents a position's value vector (with 2 features)\n",
        "\n",
        "3. **Matrix C (Result of Attention)**:\n",
        "   - Computed as A @ B (matrix multiplication)\n",
        "   - Each row of C is a weighted average of rows in B, with weights from the corresponding row in A\n",
        "   - For example, the first row of C only considers the first row of B (with weight 1)\n",
        "   - The second row of C is a weighted average of the first and second rows of B\n",
        "   - The third row of C is a weighted average of all three rows of B\n",
        "\n",
        "This simple weighted averaging is the foundation of attention mechanisms. In a full transformer model, the weights in matrix A aren't fixed - they're learned from the data based on query-key interactions, allowing the model to dynamically focus on relevant parts of the input."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Self-Attention Implementation\n",
        "\n",
        "Let's explore a simple implementation of self-attention, starting with a basic \"bag of words\" approach and building up to the full self-attention mechanism used in transformers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Toy Example Setup\n",
        "\n",
        "Let's create a toy example to demonstrate attention in a practical context. We'll create a tensor `x` with the following dimensions:\n",
        "- Batch size (B): 4 examples\n",
        "- Sequence length (T): 8 tokens per example\n",
        "- Channel dimension (C): 2 features per token\n",
        "\n",
        "This tensor could represent a batch of token embeddings in a transformer model, where each token has been mapped to a 2-dimensional vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input shape: torch.Size([4, 8, 2])\n",
            "First batch item:\n",
            "tensor([[ 0.1808, -0.0700],\n",
            "        [-0.3596, -0.9152],\n",
            "        [ 0.6258,  0.0255],\n",
            "        [ 0.9545,  0.0643],\n",
            "        [ 0.3612,  1.1679],\n",
            "        [-1.3499, -0.5102],\n",
            "        [ 0.2360, -0.2398],\n",
            "        [-0.9211,  1.5433]])\n"
          ]
        }
      ],
      "source": [
        "# Toy example setup\n",
        "torch.manual_seed(1337)  # For reproducibility\n",
        "B, T, C = 4, 8, 2  # Batch, Time (sequence length), Channels\n",
        "x = torch.randn(B, T, C)\n",
        "\n",
        "print(\"Input shape:\", x.shape)\n",
        "print(\"First batch item:\")\n",
        "print(x[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bag of Words (Mean) Attention - Manual Implementation\n",
        "\n",
        "The simplest form of attention is just averaging previous tokens. This is sometimes called a \"bag of words\" approach. Let's implement it manually first to understand the process step by step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First batch item after attention:\n",
            "tensor([[ 0.1808, -0.0700],\n",
            "        [-0.0894, -0.4926],\n",
            "        [ 0.1490, -0.3199],\n",
            "        [ 0.3504, -0.2238],\n",
            "        [ 0.3525,  0.0545],\n",
            "        [ 0.0688, -0.0396],\n",
            "        [ 0.0927, -0.0682],\n",
            "        [-0.0341,  0.1332]])\n"
          ]
        }
      ],
      "source": [
        "# Manually implement \"averaging\" attention\n",
        "xbow = torch.zeros_like(x)\n",
        "for b in range(B):  # For each batch item\n",
        "    for t in range(T):  # For each position in the sequence\n",
        "        # Average all tokens seen so far (causal attention)\n",
        "        xbow[b, t] = torch.mean(x[b, :t+1], dim=0)\n",
        "\n",
        "print(\"First batch item after attention:\")\n",
        "print(xbow[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this implementation, we're creating a new tensor `xbow` with the same shape as `x`. For each position `t` in each batch item `b`, we compute the average of all tokens from position 0 to position `t` (inclusive). This is a simple form of causal attention, where each position only considers tokens it has seen before (including itself).\n",
        "\n",
        "This nested loop approach works but is inefficient for large tensors. Let's implement the same operation using matrix multiplication, which is much faster, especially on GPUs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Matrix Multiplication for Attention\n",
        "\n",
        "We can express the same \"averaging\" attention using matrix multiplication, which is much more efficient."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attention weight matrix:\n",
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
            "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
            "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n",
            "\n",
            "Do both implementations match? False\n"
          ]
        }
      ],
      "source": [
        "# Create a weight matrix for averaging attention\n",
        "wei = torch.tril(torch.ones(T, T))\n",
        "wei = wei / wei.sum(1, keepdim=True)\n",
        "print(\"Attention weight matrix:\")\n",
        "print(wei)\n",
        "\n",
        "# Perform batch matrix multiplication\n",
        "xbow2 = wei @ x  # (T,T) @ (B,T,C) -> PyTorch broadcasts to (B,T,T) @ (B,T,C) -> (B,T,C)\n",
        "\n",
        "# Verify that our matrix implementation matches the manual one\n",
        "print(\"\\nDo both implementations match?\", torch.allclose(xbow, xbow2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This approach creates a lower triangular weight matrix `wei` where each row `i` represents the attention weights for position `i`. After normalizing, each row sums to 1, representing a proper weighted average.\n",
        "\n",
        "The matrix multiplication `wei @ x` efficiently computes the weighted average for all positions at once. PyTorch handles the batch dimension via broadcasting, making this operation very fast on GPUs.\n",
        "\n",
        "This matrix-based approach is much more efficient than nested loops and is how attention is implemented in practice. However, in a real transformer, the weights aren't fixed averaging weights - they're computed dynamically based on the content, which brings us to the next implementation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Softmax Attention\n",
        "\n",
        "In transformers, attention weights are normalized using softmax rather than simple averaging. Let's implement attention using softmax normalization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Softmax attention weights:\n",
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3610, 0.6390, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.4389, 0.2931, 0.2680, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.1660, 0.2797, 0.1742, 0.3800, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2604, 0.2253, 0.1294, 0.2615, 0.1234, 0.0000, 0.0000, 0.0000],\n",
            "        [0.1976, 0.1973, 0.1642, 0.1374, 0.1453, 0.1583, 0.0000, 0.0000],\n",
            "        [0.1887, 0.0885, 0.1245, 0.2019, 0.1046, 0.0984, 0.1933, 0.0000],\n",
            "        [0.1043, 0.1008, 0.0831, 0.1372, 0.1738, 0.0869, 0.1365, 0.1774]])\n",
            "\n",
            "Output shape: torch.Size([4, 8, 2])\n"
          ]
        }
      ],
      "source": [
        "# Create a lower triangular mask (tril) for causal attention\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "\n",
        "# Create random weights\n",
        "wei = torch.rand(T, T)\n",
        "# Apply mask: set weights to -inf where tril is 0 (future positions)\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "# Apply softmax to get normalized weights\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "print(\"Softmax attention weights:\")\n",
        "print(wei)\n",
        "\n",
        "# Perform weighted aggregation\n",
        "xbow3 = wei @ x\n",
        "print(\"\\nOutput shape:\", xbow3.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This implementation introduces two key changes:\n",
        "\n",
        "1. We use random weights instead of fixed ones, simulating the learned weights in a transformer\n",
        "2. We use softmax normalization instead of simple averaging\n",
        "\n",
        "The `masked_fill` operation sets the weights for future positions (where `tril == 0`) to negative infinity, which effectively gives them zero probability after softmax. This enforces the causal nature of the attention, ensuring that a token only attends to previous tokens.\n",
        "\n",
        "The softmax function converts the raw weights into a probability distribution (each row sums to 1), but unlike simple averaging, it can give more weight to some tokens and less to others based on their values.\n",
        "\n",
        "This approach is closer to the attention mechanism used in transformers, but it's still missing the core component: content-based attention where the weights are determined by the similarity between queries and keys."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Full Self-Attention Implementation\n",
        "\n",
        "Let's now implement the complete self-attention mechanism used in transformers, where the attention weights are computed based on the similarity between query and key vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attention output shape: torch.Size([4, 8, 16])\n"
          ]
        }
      ],
      "source": [
        "# Hyperparameters\n",
        "head_size = 16  # Dimension of query/key/value projections\n",
        "\n",
        "# Input preparation\n",
        "B, T, C = 4, 8, 32  # Larger channel dimension\n",
        "x = torch.randn(B, T, C)\n",
        "\n",
        "# Linear projections for query, key, and value\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "\n",
        "# Initialize some random weights to simulate a pre-trained model\n",
        "torch.manual_seed(1337)\n",
        "query.weight.data = torch.randn_like(query.weight.data) * 0.1\n",
        "key.weight.data = torch.randn_like(key.weight.data) * 0.1\n",
        "value.weight.data = torch.randn_like(value.weight.data) * 0.1\n",
        "\n",
        "# Generate query, key, and value projections\n",
        "q = query(x)  # (B,T,head_size)\n",
        "k = key(x)    # (B,T,head_size)\n",
        "v = value(x)  # (B,T,head_size)\n",
        "\n",
        "# Compute attention scores (\"affinities\")\n",
        "wei = q @ k.transpose(-2, -1)  # (B,T,head_size) @ (B,head_size,T) -> (B,T,T)\n",
        "\n",
        "# Scale the attention scores\n",
        "wei = wei / (head_size ** 0.5)  # Scale by sqrt(head_size)\n",
        "\n",
        "# Causal mask to ensure tokens only attend to previous tokens\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "\n",
        "# Normalize with softmax\n",
        "wei = F.softmax(wei, dim=-1)  # (B,T,T)\n",
        "\n",
        "# Weighted aggregation of values\n",
        "out = wei @ v  # (B,T,T) @ (B,T,head_size) -> (B,T,head_size)\n",
        "\n",
        "print(\"Attention output shape:\", out.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This implementation represents the full self-attention mechanism as used in transformers. Let's break down the key components:\n",
        "\n",
        "1. **Linear Projections**:\n",
        "   - We project the input `x` into three different spaces: query (q), key (k), and value (v)\n",
        "   - These projections are learnable, allowing the model to adapt what to focus on\n",
        "\n",
        "2. **Attention Scores**:\n",
        "   - We compute attention scores as the dot product between queries and keys: `q @ k.transpose(-2, -1)`\n",
        "   - This measures the similarity between each token's query and all tokens' keys\n",
        "   - High scores indicate high relevance/attention\n",
        "\n",
        "3. **Scaling**:\n",
        "   - We scale the scores by `1/sqrt(head_size)` to prevent exploding gradients in larger models\n",
        "\n",
        "4. **Causal Masking**:\n",
        "   - We apply a mask to ensure tokens only attend to previous tokens (autoregressive property)\n",
        "\n",
        "5. **Softmax Normalization**:\n",
        "   - We normalize the scores with softmax to get proper attention weights\n",
        "\n",
        "6. **Weighted Aggregation**:\n",
        "   - We use these weights to compute a weighted sum of the value vectors\n",
        "\n",
        "The key difference from our previous examples is that now the attention weights are content-based - they're determined by the similarity between token representations, allowing the model to focus on relevant parts of the input based on the content itself."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. The Complete Transformer Architecture\n",
        "\n",
        "Now that we understand self-attention, let's implement a complete transformer-based language model. We'll structure our model following the original architecture described in the \"Attention is All You Need\" paper, with a few simplifications."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Head(nn.Module):\n",
        "    \"\"\"One head of self-attention\"\"\"\n",
        "    \n",
        "    def __init__(self, head_size, n_embd, block_size, dropout):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)    # (B,T,head_size)\n",
        "        q = self.query(x)  # (B,T,head_size)\n",
        "        \n",
        "        # Compute attention scores\n",
        "        wei = q @ k.transpose(-2, -1) * (k.shape[-1]**-0.5)  # (B,T,T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))  # (B,T,T)\n",
        "        wei = F.softmax(wei, dim=-1)  # (B,T,T)\n",
        "        wei = self.dropout(wei)\n",
        "        \n",
        "        # Weighted aggregation of values\n",
        "        v = self.value(x)  # (B,T,head_size)\n",
        "        out = wei @ v  # (B,T,head_size)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `Head` class encapsulates a single attention head as we implemented above, with the addition of dropout for regularization. It takes the following parameters:\n",
        "- `head_size`: Dimensionality of the query/key/value projections\n",
        "- `n_embd`: Dimensionality of the input embeddings\n",
        "- `block_size`: Maximum sequence length (for the causal mask)\n",
        "- `dropout`: Dropout probability for regularization\n",
        "\n",
        "Next, let's create a multi-head attention module, which runs several attention heads in parallel and concatenates their outputs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"Multiple heads of self-attention in parallel\"\"\"\n",
        "    \n",
        "    def __init__(self, num_heads, head_size, n_embd, block_size, dropout):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size, n_embd, block_size, dropout) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(num_heads * head_size, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # Concatenate outputs from all heads\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        # Project back to original dimension\n",
        "        out = self.proj(out)\n",
        "        out = self.dropout(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `MultiHeadAttention` module:\n",
        "1. Creates multiple attention heads, each operating independently\n",
        "2. Concatenates their outputs along the feature dimension\n",
        "3. Projects the concatenated output back to the original embedding dimension using a linear layer\n",
        "\n",
        "This allows each head to focus on different aspects of the input, enhancing the model's representation power.\n",
        "\n",
        "Next, let's implement the feed-forward network that follows the attention layer in each transformer block:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    \"\"\"Simple feed-forward network with ReLU activation\"\"\"\n",
        "    \n",
        "    def __init__(self, n_embd, dropout):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),  # Expand to 4x dimension\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),  # Project back to original dimension\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The feed-forward network is a simple two-layer network with a ReLU activation function. It expands the input dimension by a factor of 4 in the hidden layer, following the architecture described in the original transformer paper.\n",
        "\n",
        "Now, let's implement a single transformer block, which combines multi-head attention, feed-forward network, and layer normalization:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "    \"\"\"Transformer block: communication followed by computation\"\"\"\n",
        "    \n",
        "    def __init__(self, n_embd, n_head, block_size, dropout):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size, n_embd, block_size, dropout)\n",
        "        self.ffwd = FeedForward(n_embd, dropout)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # Self-attention with residual connection\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        # Feed-forward with residual connection\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `Block` class implements a complete decoder block with:\n",
        "1. Layer normalization before each sub-layer (pre-norm variant)\n",
        "2. Multi-head self-attention\n",
        "3. Feed-forward network\n",
        "4. Residual connections around each sub-layer\n",
        "\n",
        "This follows the modern transformer architecture used in models like GPT.\n",
        "\n",
        "Finally, let's implement the full GPT language model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GPTLanguageModel(nn.Module):\n",
        "    \"\"\"GPT Language Model with transformer architecture\"\"\"\n",
        "    \n",
        "    def __init__(self, vocab_size, n_embd, n_head, n_layer, block_size, dropout):\n",
        "        super().__init__()\n",
        "        self.block_size = block_size\n",
        "        \n",
        "        # Token and position embeddings\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        \n",
        "        # Decoder blocks\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head, block_size, dropout) for _ in range(n_layer)])\n",
        "        \n",
        "        # Final layer normalization and output projection\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "        \n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        \n",
        "        # Token embeddings\n",
        "        tok_emb = self.token_embedding_table(idx)  # (B,T,C)\n",
        "        \n",
        "        # Position embeddings\n",
        "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device)  # (T)\n",
        "        pos_emb = self.position_embedding_table(pos)  # (T,C)\n",
        "        \n",
        "        # Combine token and position embeddings\n",
        "        x = tok_emb + pos_emb  # (B,T,C)\n",
        "        \n",
        "        # Apply decoder blocks\n",
        "        x = self.blocks(x)  # (B,T,C)\n",
        "        \n",
        "        # Final layer norm and output projection\n",
        "        x = self.ln_f(x)  # (B,T,C)\n",
        "        logits = self.lm_head(x)  # (B,T,vocab_size)\n",
        "        \n",
        "        # Loss calculation (if targets provided)\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "            \n",
        "        return logits, loss\n",
        "    \n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        \"\"\"Generate text by sampling from the model distribution\"\"\"\n",
        "        for _ in range(max_new_tokens):\n",
        "            # Crop input to block_size\n",
        "            idx_cond = idx[:, -self.block_size:]\n",
        "            \n",
        "            # Get predictions\n",
        "            logits, _ = self(idx_cond)\n",
        "            \n",
        "            # Focus only on the last time step\n",
        "            logits = logits[:, -1, :]  # (B, C)\n",
        "            \n",
        "            # Apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1)  # (B, C)\n",
        "            \n",
        "            # Sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
        "            \n",
        "            # Append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
        "            \n",
        "        return idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `GPTLanguageModel` class implements the complete transformer-based GPT model with the following components:\n",
        "\n",
        "1. **Embeddings**:\n",
        "   - Token embeddings: Map each token ID to a vector\n",
        "   - Position embeddings: Provide position information since the transformer has no inherent notion of sequence order\n",
        "\n",
        "2. **Transformer Blocks**: A series of transformer blocks, each containing:\n",
        "   - Multi-head self-attention\n",
        "   - Feed-forward network\n",
        "   - Layer normalization\n",
        "   - Residual connections\n",
        "\n",
        "3. **Output Layer**:\n",
        "   - Final layer normalization\n",
        "   - Linear projection to vocabulary size\n",
        "\n",
        "4. **Generation Function**:\n",
        "   - Autoregressive text generation using sampling\n",
        "   - Maintains a proper context window of maximum size `block_size`\n",
        "\n",
        "Let's initialize and train our GPT model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of parameters: 10,788,929\n"
          ]
        }
      ],
      "source": [
        "# Model hyperparameters\n",
        "n_embd = 384    # Embedding dimension\n",
        "n_head = 6      # Number of attention heads\n",
        "n_layer = 6     # Number of transformer blocks\n",
        "dropout = 0.2   # Dropout probability\n",
        "\n",
        "# Training parameters\n",
        "learning_rate = 3e-4\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "batch_size = 64\n",
        "block_size = 256  # Maximum context length\n",
        "\n",
        "# Initialize model\n",
        "model = GPTLanguageModel(vocab_size, n_embd, n_head, n_layer, block_size, dropout)\n",
        "model.to(device)\n",
        "# Calculate the number of parameters\n",
        "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Training the GPT Model\n",
        "\n",
        "Now let's train our full GPT model using the same approach as before. We'll need to update our batch generation function to handle the larger context size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Updated batch function for larger context size\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y\n",
        "\n",
        "# Initialize optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for iter in range(max_iters):\n",
        "    \n",
        "    # Evaluate loss periodically\n",
        "    if iter % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"Step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "    \n",
        "    # Sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "    \n",
        "    # Evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    \n",
        "    # Backpropagation\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This training loop is similar to our earlier one, but now we're training the full transformer-based GPT model. The training process will take longer due to the increased model size and complexity.\n",
        "\n",
        "Let's generate some text with our trained model to see the results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate text\n",
        "context = torch.zeros((1, 1), dtype=torch.long)\n",
        "print(decode(model.generate(context, max_new_tokens=500)[0].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 14. Training the GPT Model On GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Vocabulary size: 65\n",
            "Starting training...\n",
            "Step 0: train loss 4.3603, val loss 4.3642\n",
            "Step 500: train loss 1.8925, val loss 1.9951\n",
            "Step 1000: train loss 1.5392, val loss 1.7200\n",
            "Step 1500: train loss 1.3957, val loss 1.6133\n",
            "Step 2000: train loss 1.3169, val loss 1.5514\n",
            "Step 2500: train loss 1.2630, val loss 1.5193\n",
            "Step 3000: train loss 1.2096, val loss 1.4831\n",
            "Step 3500: train loss 1.1685, val loss 1.5005\n",
            "Step 4000: train loss 1.1324, val loss 1.4870\n",
            "Step 4500: train loss 1.0966, val loss 1.4758\n",
            "\n",
            "Generating sample text...\n",
            "\n",
            "\n",
            "ROMEO:\n",
            "O'er his hand; he that's answer'd for 'scent\n",
            "I am honesty, marquised, how thy hands\n",
            "Who cannot in his house to revenge him:\n",
            "But sufficility shows it is strike and before him,\n",
            "If that do that pluck him than henced my sweet,\n",
            "For hath me to bitter on fourth. Nay, his liege,\n",
            "But that 'twas time to with inking him, I'll heark.\n",
            "And so to you, my lord, lord, a prince is stoop.\n",
            "Than you show therefore's mother i' the time\n",
            "Against them, with treason shoolved bed,\n",
            "From did boting, and he is with k\n",
            "\n",
            "Training complete!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "# 1. Check for GPU and set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# 2. Load and process data\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# Create vocabulary\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(f\"Vocabulary size: {vocab_size}\")\n",
        "\n",
        "# Create mappings\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "itos = {i: ch for i, ch in enumerate(chars)}\n",
        "\n",
        "# Encoding/decoding functions\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "# Convert text to tensor and move to GPU\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "data = data.to(device)  # Move data to GPU\n",
        "\n",
        "# Split into train and validation sets\n",
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# 3. Define model architecture\n",
        "class GPTLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size, n_embd, n_head, n_layer, block_size, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.block_size = block_size\n",
        "        \n",
        "        # Token and position embeddings\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        \n",
        "        # Transformer blocks\n",
        "        self.blocks = nn.Sequential(*[\n",
        "            TransformerBlock(n_embd, n_head, block_size, dropout) \n",
        "            for _ in range(n_layer)\n",
        "        ])\n",
        "        \n",
        "        # Final layer norm and output projection\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "        \n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        \n",
        "        # Get token and position embeddings\n",
        "        tok_emb = self.token_embedding_table(idx)  # (B,T,C)\n",
        "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device)  # (T)\n",
        "        pos_emb = self.position_embedding_table(pos)  # (T,C)\n",
        "        \n",
        "        # Add embeddings\n",
        "        x = tok_emb + pos_emb  # (B,T,C)\n",
        "        \n",
        "        # Apply transformer blocks\n",
        "        x = self.blocks(x)  # (B,T,C)\n",
        "        \n",
        "        # Apply final layer norm and projection\n",
        "        x = self.ln_f(x)  # (B,T,C)\n",
        "        logits = self.lm_head(x)  # (B,T,vocab_size)\n",
        "        \n",
        "        # Calculate loss if targets provided\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "            \n",
        "        return logits, loss\n",
        "    \n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        \"\"\"Generate text by sampling from the model distribution\"\"\"\n",
        "        for _ in range(max_new_tokens):\n",
        "            # Crop input to block_size\n",
        "            idx_cond = idx[:, -self.block_size:]\n",
        "            \n",
        "            # Get predictions\n",
        "            logits, _ = self(idx_cond)\n",
        "            \n",
        "            # Focus only on the last time step\n",
        "            logits = logits[:, -1, :]  # (B, C)\n",
        "            \n",
        "            # Apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1)  # (B, C)\n",
        "            \n",
        "            # Sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
        "            \n",
        "            # Append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
        "            \n",
        "        return idx\n",
        "\n",
        "# Simple transformer block\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, n_embd, n_head, block_size, dropout):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size, n_embd, block_size, dropout)\n",
        "        self.ffwd = FeedForward(n_embd, dropout)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # Self-attention with residual connection\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        # Feed-forward with residual connection\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "# Multi-head attention\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, head_size, n_embd, block_size, dropout):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([\n",
        "            Head(head_size, n_embd, block_size, dropout) \n",
        "            for _ in range(num_heads)\n",
        "        ])\n",
        "        self.proj = nn.Linear(num_heads * head_size, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # Concatenate outputs from all heads\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        # Project back to original dimension\n",
        "        out = self.proj(out)\n",
        "        out = self.dropout(out)\n",
        "        return out\n",
        "\n",
        "# Single attention head\n",
        "class Head(nn.Module):\n",
        "    def __init__(self, head_size, n_embd, block_size, dropout):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x)    # (B,T,head_size)\n",
        "        q = self.query(x)  # (B,T,head_size)\n",
        "        \n",
        "        # Compute attention scores\n",
        "        wei = q @ k.transpose(-2, -1) * (k.shape[-1]**-0.5)  # (B,T,T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))  # (B,T,T)\n",
        "        wei = F.softmax(wei, dim=-1)  # (B,T,T)\n",
        "        wei = self.dropout(wei)\n",
        "        \n",
        "        # Weighted aggregation of values\n",
        "        v = self.value(x)  # (B,T,head_size)\n",
        "        out = wei @ v  # (B,T,head_size)\n",
        "        return out\n",
        "\n",
        "# Feed-forward network\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, n_embd, dropout):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# 4. Hyperparameters\n",
        "block_size = 256  # Context length\n",
        "batch_size = 64   # Batch size\n",
        "n_embd = 384      # Embedding dimension\n",
        "n_head = 6        # Number of attention heads\n",
        "n_layer = 6       # Number of transformer blocks\n",
        "learning_rate = 3e-4\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "dropout = 0.2\n",
        "\n",
        "# 5. Initialize model and move to GPU\n",
        "model = GPTLanguageModel(vocab_size, n_embd, n_head, n_layer, block_size, dropout)\n",
        "model = model.to(device)  # Move model to GPU\n",
        "\n",
        "# 6. Define batch generation function\n",
        "def get_batch(split):\n",
        "    # Data is already on GPU\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,), device=device)\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y\n",
        "\n",
        "# 7. Define evaluation function\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(100, device=device)\n",
        "        for k in range(100):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "# 8. Initialize optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# 9. Training loop\n",
        "print(\"Starting training...\")\n",
        "for iter in range(max_iters):\n",
        "    # Evaluate loss periodically\n",
        "    if iter % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"Step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "    \n",
        "    # Sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "    \n",
        "    # Evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    \n",
        "    # Backpropagation\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# 10. Generate sample text\n",
        "print(\"\\nGenerating sample text...\")\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "generated_ids = model.generate(context, max_new_tokens=500)[0].tolist()\n",
        "print(decode(generated_ids))\n",
        "\n",
        "print(\"\\nTraining complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 15. Conclusion and Next Steps\n",
        "\n",
        "In this notebook, we've built a GPT model from scratch, starting with the simplest bigram model and progressively adding complexity until we reached a full transformer-based architecture. Along the way, we've explored the key components that make transformers so effective:\n",
        "\n",
        "1. **Self-Attention**: The core mechanism that allows models to focus on relevant parts of the input sequence\n",
        "2. **Multi-Head Attention**: Running multiple attention operations in parallel to capture different types of relationships\n",
        "3. **Positional Encodings**: Providing sequence order information to the model\n",
        "4. **Layer Normalization**: Stabilizing training by normalizing activations\n",
        "5. **Residual Connections**: Helping with gradient flow during training\n",
        "\n",
        "Our model is a miniature version of OpenAI's GPT models, but it follows the same architectural principles.\n",
        "\n",
        "### Possible Next Steps\n",
        "\n",
        "1. **Experiment with Hyperparameters**: Try different embedding dimensions, numbers of heads, layers, etc.\n",
        "2. **Use a Different Dataset**: Train the model on a different text corpus\n",
        "3. **Implement Subword Tokenization**: Switch from character-level to subword tokenization for better performance\n",
        "4. **Add Optimizations**: Implement techniques like mixed-precision training or gradient accumulation for faster training\n",
        "5. **Fine-Tuning**: Train the model on a specific task after pre-training\n",
        "\n",
        "Understanding how these models work from the ground up gives you a solid foundation for working with modern language models like GPT, BERT, and others. Happy exploring!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "env_ml",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
